{"pages":[],"posts":[{"title":"分布式集群 - 创建一个索引","text":"##Add an Index 要向 Elasticsearch 添加数据, 我们就需要一个索引 – 一个存放相关数据的地方, 事实上索引只是一个逻辑名字空间指向一个或者多个物理分片 分片是一个底层工作段元, 它支持有索引中数据的一部分, 在 inside-a-shard 这个小节中我们会解释其工作细节, 但是现在我们只需要知道一个分片只是一个单一的 Lucene 实例并且它自身也是一个完整的搜索引擎就可以了, 我们的文档存储和索引在这些分片中, 但是我们的应用程序不会直接和他们通讯, 对于应用来说仅仅操作索引本身就足够了. 分片是 Elasticsearch 将数据分布到整个集群的手段, 可以将分片们想象成数据的容器, 文档被存储在分片中, 分片们被分配到你集群的节点中, 当你集群扩展或者收缩时, Elasticsearch 将会自动在节点之间调整这些分片的位置来位置集群的均衡性. 一个分片可以是主分片或者复制分片, 在你索引中所有的文档都会属于某个主分片, 所以你选择的主分片的数量就决定了, 你这个索引最多能存放多少数据. 虽然主分片数据可以存放的数据没有理论上的限制, 但是依然有一些实际的限制, 最大分片大小完全取决于你的使用场景: 你所拥有的硬件资源, 文档的数量和复杂度, 如何索引和查询你的文档和你期待的返回时间 复制分片仅仅只是朱分片的副本, 复制集用于提供冗余的备份以避免硬件损坏和处理读取请求比如搜索或者获取文档. 索引中主分片的数量是在索引创建时就确定了的, 但是复制分片的数量可以在任何时候被改变. 让我们在我们的单节点空集群中创建一个名为 blogs 的索引, 默认情况下, 索引将会被分配5个主分片, 但我们这里仅仅是一个实例, 我们将会只分配三个主分片和一个复制分片(对于每个主分片一个复制分片, 这里复制分片总数量为3) PUT /blogs { &quot;settings&quot; : { &quot;number_of_shards&quot; : 3, &quot;number_of_replicas&quot; : 1 } } 我们集群现在由一个单节点和一个索引构成, 所有三个朱分片都被分配在节点 1上面 如果我们现在检查cluster-health, 我们将会看到: { &quot;cluster_name&quot;: &quot;elasticsearch&quot;, &quot;status&quot;: &quot;yellow&quot;, (1) &quot;timed_out&quot;: false, &quot;number_of_nodes&quot;: 1, &quot;number_of_data_nodes&quot;: 1, &quot;active_primary_shards&quot;: 3, &quot;active_shards&quot;: 3, &quot;relocating_shards&quot;: 0, &quot;initializing_shards&quot;: 0, &quot;unassigned_shards&quot;: 3 (2) } 集群状态为 yellow 我们三个副本分片没有被分配到一个节点上 一个集群健康度为yellow意味着所有的主分片已经启动并运行(集群已经可以接受任何请求并成功返回)但是不是所有的复制分片可用, 事实上, 我们的三个复制分片现在是unassigned – 他们并没有被分配到任何一个节点上, 这是因为将复制的数据放到同样的节点上没有任何实际意义, 如果这个节点出问题, 我们还是会丢失所有数据 现在, 我们的集群已经完全可以使用, 但是存在丢失数据的风险.","link":"/2014/12/25/EDG-Distributed-Cluster-Add-an-index/"},{"title":"关于Akka中集群Pool的一些行为","text":"以集群方式创建一个pool是一种非常方便的方式, 它能够根据配置自动向相关节点部署actor并管理, 但是并非其所有行为都在文档中给与了描述, 因此本文将在一些容易产生问题的部分进行补充. ####奇怪的概念 ClusterRouterPoolActor 继承于 RouterPoolActor 但未重写管理消息处理逻辑, 因此会导致Terminated和AdjustPoolSize两个管理消息不能正常运行, 他们在执行时均未考虑到节点选择逻辑, 将会以不确认顺序的方式从所有运行Routee的节点中随机移除或调整Actor造成节点负载失衡. ClusterRouterPool 不支持 Resizer ####部署Routee 寻找节点 - 首选判断目前部署的Routee总数是否已达到上限, 然后寻找目前数量最少的节点, 如果目前数量最少的节点达到了maxInstancesPerNode限制, 则不做任何部署. 部署Actor - 在寻找到能够部署Actor的节点之后, 创建一个基于目标地址的Deploy然后使用工厂方法和deploy创建pool的Routee, 将创建好的Routee放入cell中以备查询. 递归调用直到没有能够部署的节点为止 #### akka monitor &amp; observer design","link":"/2015/02/11/AKKA-About-Cluster-Pool/"},{"title":"分布式集群 - 增加失效备援","text":"##Add Failover 运行单节点意味着没有任何冗余, 幸运的是, 我们可以启动另一个节点来防止这种数据丢失 启动第二个节点为了测试当你加入第二个节点时会发生什么事情, 你可以安全按照你启动第一个节点的方式启动新的一个节点, 使用同一个目录, 多个节点可以共享使用同样的目录.只要第二个具备和第一个节点相同的cluster.name(在./config/elasticsearch.yml), 它将自动寻找并加入第一个节点所启动的集群, 如果它没有这样做, 检查日志并找到错误原因, 可能是你的局域网禁止多播, 或者防火墙阻止了你节点之间的通讯 当第二个节点启动之后 一个双节点集群中所有的主分片和复制分片都成功分配 第二个节点已经加入集群, 并且三个复制分片都已经成功分配到节点上, 这意味着我们如果失去任意一个节点, 任何数据都不会丢失 任何新被索引的文档都将首先存储到主分片上, 然后并行的被复制到相关的复制分片中, 这确保了我们的文档可以从主分片或者它的复制分片获取. cluster-health现在status为green了这意味着所有的六个分片都已激活(三个主分片和三个复制分片): { &quot;cluster_name&quot;: &quot;elasticsearch&quot;, &quot;status&quot;: &quot;green&quot;, (1) &quot;timed_out&quot;: false, &quot;number_of_nodes&quot;: 2, &quot;number_of_data_nodes&quot;: 2, &quot;active_primary_shards&quot;: 3, &quot;active_shards&quot;: 6, &quot;relocating_shards&quot;: 0, &quot;initializing_shards&quot;: 0, &quot;unassigned_shards&quot;: 0 } 集群状态为green 我们的集群现在不仅仅是可以工作了, 并且具备高可用性","link":"/2014/12/25/EDG-Distributed-Cluster-Add-failover/"},{"title":"分布式集群 - 集群健康度","text":"##Cluster health Elasticsearch 集群中有许多统计可以被监控, 但是最重要的一项则是集群健康度, 通过报告status为green,yellow,或者red GET /_cluster/health 一个没有索引的空集群, 其健康度应该是看起来这样: { &quot;cluster_name&quot;: &quot;elasticsearch&quot;, &quot;status&quot;: &quot;green&quot;, (1) &quot;timed_out&quot;: false, &quot;number_of_nodes&quot;: 1, &quot;number_of_data_nodes&quot;: 1, &quot;active_primary_shards&quot;: 0, &quot;active_shards&quot;: 0, &quot;relocating_shards&quot;: 0, &quot;initializing_shards&quot;: 0, &quot;unassigned_shards&quot;: 0 } status 字段通常是最需要关注的 status 字段从整体上提供了一个集群的工作状态, 三种颜色的状态意义如下: green 所有主备分片均工作正常 yellow 所有主分片工作正常, 但是不是所有备份分片工作正常 red 不是所有的主分片工作正常 在这个章节的接下来部分, 我们将会解释主备分片和每种状态实际的意义","link":"/2014/12/25/EDG-Distributed-Cluster-Cluster-health/"},{"title":"分布式集群 - 空集群","text":"##An Empty Cluster如果我们启动了一个节点并且没有任何数据和索引, 我们的集群看起来就像包含了一个空节点的集群. 一个节点运行了 Elasticsearch 实例, 当一个集群具备一个或者多个节点使用同一个cluster.name他们就会共享他们的数据和负载, 每当一个节点在集群中被加入或者被移除, 集群会调整自身使得数据重新被平均分配. 集群中的一个节点将会被选举成为主控节点, 这个节点将会负责管理集群层面的变动, 比如增加或者删除索引, 添加或者移除节点, 主控节点和文档层面的改变或者搜索不会产生关系, 这使得主控节点不会成为数据传输的瓶颈, 任何节点都可以成为主控节点, 我们的示例集群中只有一个节点, 所以理所当然它承担了主控节点的角色. 作为用户, 我们可以和集群中任意节点进行通讯, 包括主控节点, 每个节点都知道所有文档的具体为止, 并且可以将我们的请求重定向到那个包含我们感兴趣数据的节点, 无论我们请求的那个节点结果都将由这个节点返回, 这个节点自己会控制整个请求过程, 这一切都由 Elasticsearch 管理并对客户端透明.","link":"/2014/12/25/EDG-Distributed-Cluster-Empty-cluster/"},{"title":"分布式集群 - 水平扩展","text":"##Scale Horizontally What about scaling as the demand for our application grows? If we start a third node, our cluster reorganizes itself to look like A three-node cluster—shards have been reallocated to spread the load. A three-node clusterFigure 1. A three-node cluster—shards have been reallocated to spread the loadOne shard each from Node 1 and Node 2 have moved to the new Node 3, and we have two shards per node, instead of three. This means that the hardware resources (CPU, RAM, I/O) of each node are being shared among fewer shards, allowing each shard to perform better. A shard is a fully fledged search engine in its own right, and is capable of using all of the resources of a single node. With our total of six shards (three primaries and three replicas), our index is capable of scaling out to a maximum of six nodes, with one shard on each node and each shard having access to 100% of its node’s resources.","link":"/2014/12/25/EDG-Distributed-Cluster-Scale-horizontally/"},{"title":"相关度 - 简介","text":"Controlling relevance数据库使用单纯的结构化数据(比如日期,数字,文本和枚举)处理起来很容易 - 他们只需要检查一个文档(对于关系型数据库来说则是一行)是否匹配查询条件即可.虽然匹配和不匹配是全文搜索不可缺少的一部分, 但是仅仅是这样是不够的, 取而代之的, 我们需要在这次查询中知道每个文档有多相关. 全文搜索引擎不仅仅是找到这些匹配的文档更进一步的还会根据相关度进行排序. 全文相关度公式或者说相似度算法, 为每个文档合并一系列的因素来计算出一个相关度分数(_ score). 在这一章, 我们将深入了解各部分的细节并且讨论如何来进行控制.当然, 相关度不仅仅关系到全文检索, 结构化数据同样需要考虑到这一点, 假如我们正在根据一些特定的功能(空调,海景,免费 wifi)来寻找假日酒店. 一个房子包含越多我们需要的功能, 那么它的相关度就越高, 或者加入我们需要额外的刻度(sliding-scales)比如年限, 价格, 欢迎程度或者距离,这些在进行全文检索计算相关度的时候都需要考虑进去. 还好 Elasticsearch 中具备了强大的评分基础使得这一切成为了可能. 我们将开始从 Lucene 如何计算相关度的理论开始, 然后通过一些例子来告诉你如何控制这些过程.","link":"/2014/10/12/EDG-Relevance-Intro/"},{"title":"相关度 - Lucene 实质评分方法","text":"Lucene’s Practical Scoring Function对于多片段查询, Lucene 使用布尔模型, 片段频率/逆文本频率和空间向量模型并且将他们结合成一个单一有效的整体, 来收集匹配的文档和计算出他们的分数. 一个多片段查询, 通常看起来是这样的: GET /my_index/doc/_search { &quot;query&quot;: { &quot;match&quot;: { &quot;text&quot;: &quot;quick fox&quot; } } } 系统内部将把这次查询重写为: GET /my_index/doc/_search { &quot;query&quot;: { &quot;bool&quot;: { &quot;should&quot;: [ {&quot;term&quot;: { &quot;text&quot;: &quot;quick&quot; }}, {&quot;term&quot;: { &quot;text&quot;: &quot;fox&quot; }} ] } } } bool 查询实现了布尔模型, 并且在这个例子中将会找到包含 quick 或者 fox 中任何一个或两者都具备的文档. 当一个文档一旦被匹配之后, Lucene使用一个叫做实质评分方法(Practical Scoring Function)将为这次查询计算这个文档的分数, 将每个片段的分数进行合并, 名字看起来有点吓人, 但是不用感到不安, 其中用到的大部分组件都是你已经知道了的, 它仅仅引入了很少一些新的元素, 我们接下来将会对其进行讨论. score(q,d) = (1) queryNorm(q) (2) · coord(q,d) (3) · ∑ ( (4) tf(t in d) (5) · idf(t)² (6) · t.getBoost() (7) · norm(t,d) (8) ) (t in q) (4) score(q,d) 用于计算在使用查询 q 来查询文档 d 时候的相关度分数的函数 queryNorm(q) 查询归一化因子(query normalization factor, 新概念) coord(q,d) 协调因子(coordination factor, 新概念) 查询 q 中 每个片段 t 在匹配文档 d 的权重合计 tf(t in d) 片段 t 在文档 d 中的片段频率 idf(t) 片段 t 在整个索引中的逆文本频率 t.getBoost() 片段 t 在此次查询中被使用的增强(boosting, 新概念) norm(t,d) 文档 d 的字段长度基准, 并且与索引时字段级别增强(index-time field-level boost, 新概念)合并计算 你应该已经了解评分, 片段频率和逆文本频率, queryNorm, coord, t.getBoost 和 norm 则包含了新的概念. 我们将会在这章晚一些的时候讨论更多关于查询时增强的细节, 但是让我们先搞定查询归一化, 协调化, 协调化和索引时字段级增强这些概念. 查询归一化因子(Query normalization factor)查询归一化因子尝试”归一”一个查询, 让其结果能够和其他结果进行比较 虽然查询归一的目的是使不同的查询结果具有可比性, 但是通常不是很理想, 事实上, 相关性分数是目前能够使查询结果能够正确排序的手段, 你不应该视图比较不同查询所产生的相关性分数 这个因子在查询开的时候被计算出来, 事实上计算过程依赖于所涉及的查询, 一个经典的实现如下: queryNorm = 1 / √sumOfSquaredWeights (1) sumOfSquaredWeights 是将这次查询中所有片段的逆文本频率相加并进行平方 相同的查询归一化因子将会附加给所有文档并且你没有任何办法去改变它, 总而言之, 它可以被设置成忽略 查询协调化(Query coordination)查询协调因子(coord)用于提高包含所查询片段比率高的文档分数, 文档中出现了越多的查询的片段, 则这个文档有更大机会是一个有效的匹配. 假设我们有一个查询包含了quick brown fox这三个片段, 他们每个词的权重为1.5, 如果没有进行协调化, 他们的分数只是在文档中这些片段权重的汇总, 例如: fox → 分数: 1.5 quick fox → 分数: 3.0 quick brown fox → : 4.5 协调因子乘以匹配的片段在这个文档中的分数然后除以这个查询中总的片段数, 在加上协调因子之后, 分数就有很大的不同了: fox → score: 1.5 * 1 / 3 = 0.5 quick fox → score: 3.0 * 2 / 3 = 2.0 quick brown fox → score: 4.5 * 3 / 3 = 4.5 加入了协调因子的结果, 当同时包含了3个片段的文档就比仅仅包含2个片段的文档相关度高得多. 不要忘记包含quick brown fox的查询是被重写成了如下的布尔查询: GET /_search { &quot;query&quot;: { &quot;bool&quot;: { &quot;should&quot;: [ { &quot;term&quot;: { &quot;text&quot;: &quot;quick&quot; }}, { &quot;term&quot;: { &quot;text&quot;: &quot;brown&quot; }}, { &quot;term&quot;: { &quot;text&quot;: &quot;fox&quot; }} ] } } } bool 查询在默认情况下使用should 下所有的片段作为查询协调因子, 但是也允许你屏蔽掉协调因子, 为什么需要这样做呢? 呵呵, 通常的答案是: 不需要! 查询协调因子通常都是起到正面的作用, 当你使用bool 查询来包装一些高级别(high-level)的查询比如 match查询, 这种情况下保持协调化被开启也是有意义的, 越多的匹配, 你搜索结果之间的重叠度就更高. 然而依然有一些复杂的场景可能需要关闭协调化, 假设你在搜索一些同义字, 比如jump, leap 和 hop, 你不关心这些同义词在文档中命中了多少次, 因为他们都表示相同的意思, 事实上, 通常只有一个同义词会被命中, 这是一个适合关闭协调因子的合适场景: GET /_search { &quot;query&quot;: { &quot;bool&quot;: { &quot;disable_coord&quot;: true, &quot;should&quot;: [ { &quot;term&quot;: { &quot;text&quot;: &quot;jump&quot; }}, { &quot;term&quot;: { &quot;text&quot;: &quot;hop&quot; }}, { &quot;term&quot;: { &quot;text&quot;: &quot;leap&quot; }} ] } } } 事实上, 当你使用同义词时, 内部将会进行处理, 重写请求为同义词关闭协调化, 大部分这种关闭动作会自动进行执行(但是中文环境缺少同义词词库的情况下可能不会执行), 你并不需要担心这些. 索引时字段级增强(Index time field-level boosting)我们将对字段的增强进行讨论, 通过标记使得某些字段比其他字段更加重要, 使用查询时增强(query-time-boosting)这个晚点会进行说明, 另一种办法是在索引的时候来进行增强, 事实上, 这个增强会应用到字段里面的每一个片段而不是字段本身. 为了在保存增强结果而不导致索引占据更多的空间, 索引字段级增强将会把结果合并到字段长度基准(field length norm)中通过一个字节来存储, 通过上面提到的norm(t,d)方法返回结果. 我们基于以下几个原因强烈反对建议使用索引时字段增强: 合并增强结果到字段长度基准使用一个字节进行存储意味着字段长度基准会损失一些精度, 结果是Elasticsearch无法区分一个字段有3个单词或者5个单词 如果希望修改索引时的字段增强结果, 你必须对所有文档进行索引重建, 查询时增强则可以在每次查询的时候变化. 如果一个被索引时增强的字段具备多个值(数组), 增强会被执行多次并相乘, 导致这个字段的权重非正常的提高查询时增强更加简单,干净具备更加灵活的选项 现在查询归一化, 协调化和索引时字段增强我们都讨论完毕了, 接下来我们可以讨论可能是相关度计算中最有用的工具了, 查询时字段增强","link":"/2014/11/12/EDG-Relevance-Practical_scoring/"},{"title":"分布式集群 - 简介","text":"##Life Inside a Cluster ###追加章节就像之前提到的, 这个追加章节(除此之外还有一些其他主题的追加章节)是关于 Elasticsearch 如何在分布式集群环境下工作. 在这个章节我们会解释一些常常会用到的术语比如, 集群, 节点和分片, 还有Elasticsearch 集群进行横向扩展和如何应对硬件故障的方法 虽然这个章节是选读的 (因为你有可能在很长一段时间内使用 Elasticsearch 不需要关心分片, 副本和失效备援) 但是可以帮助你理解 Elasticsearch 内部是如何工作的, 你完全可以跳过此章节并在有需要的时候再回来读它 Elasticsearch 被构建为高可用性并且可以依照你得需求进行扩展, 你可以通过购买更加强大的服务器(垂直扩展)或者购买更多的服务器(水平扩展)来进行扩展. 虽然 Elasticsearch 可以从强大的硬件获得性能提升, 但是垂直扩展是有其限制的, 真正的扩展都是通过水平进行的, 增加更多节点到集群并且在这些节点之间分配负载和可靠性需求 大部分数据库进行水平扩展的时候通常需要对应用程序进行大量修改, 与此相反, Elasticsearch 天生就是分布式的, 它知道如何管理多节点来提供扩展性和高可用性, 这意味着你得应用程序完全不需要关注这些. 在这个章节, 我们将会告诉你如何根据你得需求配置集群, 节点和分片, 这样即使遇到硬件损坏, 你的数据也是安全的.","link":"/2014/12/02/EDG-Distributed_Cluster-Intro/"},{"title":"相关度 - 查询时增强","text":"Query time boosting在这个章节我们将介绍在查询时如何使用 boost 参数来区分字段的重要性, 从而使搜索结果得到增强 GET /_search { &quot;query&quot;: { &quot;bool&quot;: { &quot;should&quot;: [ { &quot;match&quot;: { &quot;title&quot;: { &quot;query&quot;: &quot;quick brown fox&quot;, &quot;boost&quot;: 2 (1) } } }, { &quot;match&quot;: { (2) &quot;content&quot;: &quot;quick brown fox&quot; } } ] } } } 针对 title 字段的查询子句的重要度是 content 字段的两倍, 因为它被因子2所增强 没有使用增强参数的查询, 默认情况下增强值为1 查询时增强是我们调整相关度的主要工具, 任何类型的查询都可以使用增强参数, 当一个增强参数被设置为2时不仅仅是将最终的_ score 变为两倍, 事实上增强参数被通过标准化和一些内部的优化之后才会生效, 但是, 确实增强2的情况意味着两倍重要于增强1 事实上, 对于一个查询没有一个简单的公式可以帮你选择正确的增强参数, 你基本上只能边试边看, 记住增强(boost)只是相关度分数众多因子中得一个 - 他们还需要其他因子一起进行计算, 比如之前提到的例子, title 字段可能已经自然而然的因为字段长度因子被增强 - 标题通常总是比内容长度短, 所以不要盲目的去增强一个字段只是因为你觉得他们应该被增强, 添加一个增强看看结果, 然后再换一个增强再看看, 确保最终结果是你想要的. 增强索引(本身)当跨索引进行搜索的时候, 你可以使用indices_boost参数增强整个索引, 在下面的例子中, 通过这种方式给时间较近的索引较高的权重: GET /docs_2014_*/_search (1) { &quot;indices_boost&quot;: { (2) &quot;docs_2014_10&quot;: 3, &quot;docs_2014_09&quot;: 2 }, &quot;query&quot;: { &quot;match&quot;: { &quot;text&quot;: &quot;quick brown fox&quot; } } } 这个多索引查询包括了所有以docs_2014_开头的索引 所有在索引docs_2014_10中得文档将被增强3, 那些在索引docs_2014_09被增强2, 其他的匹配的索引将保持默认增强值1 t.getBoost()在之前的Lucene 实质评分方法章节中提到过这个因子, 这些增强无法应用在查询表达式的层面上, 但是, 任何被增强结果都将被合并和传递到某个片段上, t.getBoost()函数返回任何被应用于片段本身或者查询的增强值. 事实上, 直接去阅读 explain 输出的结果稍微有点复杂, 你只能看到合并了增强的查询基准(Query Norm)的片段, 而完全无法在 explain 中看到增强值或者 t.getBoost(), 尽管我们说对于所有片段都是使用相同的查询基准方法, 你将会看到哪些增强了之后片段权重会比为增强的要高.","link":"/2014/11/24/EDG-Relevance-Query_time_boosting/"},{"title":"Elasticsearch or Solr","text":"##功能ES 和 Solr 都是基于Lucene 的搜索引擎, 对于我们使用来说, 无论选择哪一个都能满足我们的核心需求, 但是, 根据经验每个系统在设计和实现的时候必然面临选择专注于某些领域, 受限制于精力,成本, 这样一来就必然意味着在某些领域的不足. Solr由始自终主要在解决信息检索的问题, 这可以从它的 API 上反映出来, 在这方面 Solr 比 ES 更为强大, 对于 ES 来说从名字就可以看出来它专注于弹性(elasticity), 在信息检索方面则稍微有点落后, 在我们的场景中, 我们通常不会用到很多高级功能(就目前和规划中的功能而言), 虽然 Solr 在这方面具备优势, 但事实上在面临海量数据的现状下, 建议还是选择 ES. ##搜索扩展性由于之前对 Solr 进行过一段时间的研究, 我们大致上已经明白它是如何进行工作也知道了它的一些限制, ES 则是另一种尝试, 我们确信我们将要面临的数据, 不是两三台机器就可以搞定的了. 在这个级别上我们需要新的技术, 我们会有一个大规模集群采集大量数据而集群需要近乎实时的去处理他们, 将他们存储, 索引同时应对应用层的查询请求. 使用 Solr 我发现大量新的一些特性还不成熟, 特别是Overseer/Queue管理方面, 实际上很多使用者反映 Solr 分布式下非常不稳定, 在大量的测试中发现整个集群被全部锁定需要重启整个集群才能解决. ES 在压力下只出现一些非不可恢复性错误, 我尝试强制让 ES 丢失某个分片中的一些数据, 但是 ES 可以很好的从副本中恢复. ES 的团队倾尽全力在系统弹性上, 特别是以下几点, Solr 中 Collection API 是全新并且非常简陋, 然而ES 有native, 强大和经过实战检验的索引管理系统. Solr 和 ES 都有将文档分配到集群节点上的可靠手段, 但 ES 的路由框架较之Collection API 则要稳定和强大的多. 目前针对 ES 的主从模型和 Solr 的分布式模型已经有了很多讨论, 结论通常认为实现质量远比实现的理论更加重要. ##性能ES 尽管在使用 Lucene 4 之前性能较之Solr 较低, 但是较新版本的 ES 性能已经和 solr 不相上下. ##社区ES 和 Solr 都是开源项目并且都有一个活跃的社区. ##其他方面还有一些理由是考虑使用 ES 的因素: ES API 更加强大和优美, 作为一个 RESTful Webservice 也非常容易理解. 甚至可以供前端程序直接调用. Scan 和 Prelocate 功能非常有趣, 一些潜在的功能很可能会用到他们 ES 支持自动类型识别非常方便, 同时提供的配置也非常全面 ES plugin 开发难度较低, 并且已经有了非常多的 plugin 可供直接使用/参考 Real-time search ##测试稍微晚点我会设计性能测试用例, 并且将测试结果公布.","link":"/2014/11/28/Elasticsearch_or_solr/"},{"title":"Elasticsearch 安装配置以及性能评估方法","text":"Elasticsearch 是一个实时的分布式搜索和分析引擎. 它可以帮助你用前所未有的速度去处理大规模数据.它可以用于全文搜索,结构化搜索以及分析，当然你也可以将这三者进行组合 维基百科使用 Elasticsearch 来进行全文搜做并高亮显示关键词，以及提供search-as-you-type、did-you-mean等搜索建议功能. 英国卫报使用 Elasticsearch 来处理访客日志，以便能将公众对不同文章的反应实时地反馈给各位编辑. StackOverflow 将全文搜索与地理位置和相关信息进行结合,以提供more-like-this相关问题的展现. GitHub 使用 Elasticsearch 来检索超过 1300 亿行代码. Goldman Sachs 使用它来处理每天5TB数据的索引, 还有很多投行使用它来分析股票市场的变动. ##安装 ES确保服务器已经安装 JDK 1.7 下载 http://www.elasticsearch.org/overview/elkdownloads/得到 elasticsearch-version.zip 解压缩到 elasticsearch-version 我们在下文把他成为$ELASTIC_HOME 可以尝试以默认参数运行 $ELASTIC_HOME/bin/elasticsearch 访问 localhost:9200 确认搜索引擎工作正常(retCode=200) ##安装Analyser将 IK Analyser 克隆到本地 https://github.com/medcl/elasticsearch-analysis-ik.git执行 mvn clean package将target/releases/elasticsearch-analysis-ik-1.2.9下所有内容拷贝到 $ELASTIC_HOME/plugins/analysis-ik将config拷贝到$ ELASTIC_HOME/config修改elasticsearch.yml #cluster.name: elastic node.name: &quot;localhost&quot; node.master: true index: analysis: analyzer: ik: alias: [news_analyzer_ik,ik_analyzer] type: org.elasticsearch.index.analysis.IkAnalyzerProvider ik_max_word: type: ik use_smart: false ik_smart: type: ik use_smart: true GET localhost:9200/test/_analyze?analyzer=ik&amp;pretty=true&amp;text=测试分词结果确认分词结果正常 { &quot;tokens&quot; : [ { &quot;token&quot; : &quot;测试&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 2, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 1 }, { &quot;token&quot; : &quot;分词&quot;, &quot;start_offset&quot; : 2, &quot;end_offset&quot; : 4, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 2 }, { &quot;token&quot; : &quot;词&quot;, &quot;start_offset&quot; : 3, &quot;end_offset&quot; : 4, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 3 }, { &quot;token&quot; : &quot;结果&quot;, &quot;start_offset&quot; : 4, &quot;end_offset&quot; : 6, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 4 } ] } ##创建索引PUT localhost:9200/test?pretty ####创建测试索引schemaPUT localhost:9200/test/_mapping/document/ { &quot;document&quot;: { &quot;_all&quot;: { &quot;index&quot;: &quot;not_analyzed&quot;, &quot;store&quot;: &quot;false&quot; }, &quot;properties&quot;: { &quot;id&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;index&quot;: &quot;not_analyzed&quot; }, &quot;tags&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;store&quot;: &quot;no&quot;, &quot;indexAnalyzer&quot;: &quot;ik&quot;, &quot;searchAnalyzer&quot;: &quot;ik&quot; }, &quot;geo&quot;: { &quot;type&quot;: &quot;geo_point&quot; }, &quot;content&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;indexAnalyzer&quot;: &quot;ik&quot;, &quot;searchAnalyzer&quot;: &quot;ik&quot; }, &quot;date&quot;: { &quot;type&quot;: &quot;date&quot;, &quot;format&quot;: &quot;dateOptionalTime&quot; } } } } ####测试插入数据PUT localhost:9200/test/_mapping/document/1 { &quot;id&quot;: &quot;1&quot;, &quot;tags&quot;: [ &quot;sports&quot;, &quot;music&quot; ], &quot;geo&quot;: { &quot;lat&quot;: 31.267, &quot;lon&quot;: 120.872 }, &quot;content&quot;: &quot;this is only for test&quot;, &quot;date&quot;: &quot;2014-04-24T18:29:19Z&quot; } 返回 { &quot;_index&quot;: &quot;test&quot;, &quot;_type&quot;: &quot;document&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 1, &quot;created&quot;: true } ####测试搜索GET localhost:9200/test/document/_search返回 {&quot;took&quot;:285,&quot;timed_out&quot;:false,&quot;_shards&quot;:{&quot;total&quot;:5,&quot;successful&quot;:5,&quot;failed&quot;:0},&quot;hits&quot;:{&quot;total&quot;:1,&quot;max_score&quot;:1.0,&quot;hits&quot;:[{&quot;_index&quot;:&quot;test&quot;,&quot;_type&quot;:&quot;document&quot;,&quot;_id&quot;:&quot;1&quot;,&quot;_score&quot;:1.0,&quot;_source&quot;:{ &quot;id&quot;: &quot;1&quot;, &quot;tags&quot;: [ &quot;sports&quot;, &quot;music&quot; ], &quot;geo&quot;: { &quot;lat&quot;: 31.267, &quot;lon&quot;: 120.872 }, &quot;content&quot;: &quot;this is only for test&quot;, &quot;date&quot;: &quot;2014-04-24T18:29:19Z&quot; }}]}} ##集群部署在一个网域内相同名字的 Elasticsearch 服务器会被作为集群使用, 作为使用方, 你可以访问任意节点来获取数据 ####集群健康度GET /_cluster/health随时确保集群状态为 green","link":"/2014/11/30/Elasticsearch_setup_and_test/"},{"title":"Elastic Search 自定义公式","text":"由于最早编写公式时较为随意没有特别注意性能, 导致涉及大数据量时需要几秒甚至十几秒才能返回结果. 因此在编写 scoring function 有一些需要特别注意的方面(以 java 为例, 适用于 java native 以及 groovy): 数据类型运算效率排序: int &gt; short &gt; byte &gt; long &gt; float = double 运算效率在不同的数据类型有一些差异, 但是大致上可以假设(除了浮点数)加减法开销为1, 乘法为5, 除法为50 移位为8.5 布尔运算在1-2之间 浮点数乘法开销大于除法, 约是一倍, 浮点数的除法所需要的运算时间大约是int 类型加减法的350倍 避免操作日期类型或者其他复杂数据类型 对于比较固定的值, 个人建议可以提前计算好放进索引中, 但是相对就没那么灵活, 如果计算策略修改势必大规模重建索引 另一种解决方案是使用 java native script 其实就是开发一个 plugin 以内置函数的方法来提供服务, 但是依然要避免上述的性能问题 最终的解决方案还是用 filter 避免同时计算大量的分数, 还可以考虑 ES 所提供的 rescoring 来对高匹配结果进行重新计算分数排序","link":"/2014/12/25/elasticsearch-custom-scoring-notice/"},{"title":"Elasticsearch Tips","text":"强制一致性, 从primary shard获取数据, preference=primary_first 直接调用相关索引的分析器, /index/_analyze?analyzer=ik_smart想到再补充 查看服务器正在运行的线程, /_nodes/prmes01/hot_threads","link":"/2015/04/27/elasticsearch-tips/"},{"title":"使用Doc Values降低heap开销","text":"Elasticsearch 不仅仅是一个全文搜索引擎, 而且很多用户根本不把elasticsearch作为全文搜索引擎来使用而是用于facet(aggs)分析, 这是完全没有问题的, 但是你可能已经知道, 对一个字段的facet和sorting需要将field values全部装在到内存中称之为doc values的数据结构, 在通常的情况下这些field data会使用数十G内存, 现在内存成本不是很高, 通常来说在有足够的内存的情况下这不会引发什么问题, 但是在jvm层面上会引发一些问题, 一个主要的垃圾收集行为在一个具备数十G heap的java线程中会引发数秒的停顿, 这样会导致应用程序在这次查询非常的缓慢, 优化jvm可以缓解这个问题, 但是更好的办法是让这些field不再存储在heap中. 使用doc values通常会导致查询和聚合性能下降10%-25%, 这取决于操作系统文件缓存和磁盘速度, 由于不同的硬件条件, 建议在使用doc values先对现有数据进行充分的测试. ###使用Doc ValuesDoc Values 功能是在elasticsearch 1.0 之后引入的, 详细的信息可以查询当时的release note Doc Values 是Lucene 4.x 的新功能, 它允许你将field values以列模式存储在硬盘, 有效的利用文件系统缓存来完成自定义评分, 排序, 和facet(aggs), 基本上field data一致. 要使用doc values你必须在一开始创建这个字段映射的时候就将其指定为doc values, 无论是针对inverted还是uninverted索引, 这种新的实现对比传统的field data具备一些优势. jvm heap 缓存被基于操作系统级别的文件系统缓存代替, 避免 GC带来的时间开销和cpu开销 doc values将在索引的时候被计算, 所以refresh将会更快 存储到磁盘上的doc values将会被更有效的压缩 从另一方面来说, doc values会导致索引文件变得更加大. ###什么时候应该使用Doc ValuesDoc Values 在大部分 uninverted 索引字段的情况下可以代替 field data, 对于一些特定的场景更加适合. 有限的硬件, heap中的field data结构会消耗掉大量的内存, 而节点的内存并不是无限制增加的, 但是数据却会不断的增加. 周期运行的非实时性任务, 有些场景我们需要定期的运行一些聚合类查询来进行统计或达到一些其他的目的, 在这些情况下查询的延迟和吞吐量并不是最重要的, 把相关字段作为field data存储在内存中不是特别有意义, 因为他们并不是会频繁访问的. 内存管理, 将elasitcsearch消耗的大部分内存转移到磁盘可以让elasticsearch以一个很小的heap来运行, 让文件系统和操作系统来管理内存我们不会遇到任何内存溢出, 内存不足的问题, 同时也没有GC停顿的问题. 通常基于一段文本的分词索引称之为inverted索引, 而not_analysed的字符串和数字为uninverted索引, 具体的实现细节和区别, 请参考官方文档. ###如何打开Doc ValuesDoc Values是一个索引时的选择, 所以他们需要在创建mapping的时候就被选择, 这里有个例子. &quot;my_field&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;index&quot;: &quot;no&quot;, &quot;fielddata&quot;: { &quot;format&quot;: &quot;doc_values&quot; } } 就目前而言, doc values支持的non-analysed的string字段和所有的数字字段(byte, short, integer, long, float, date, double), 如你所见, 要使用doc values 不一定需要进行索引, 并且一旦当doc values被打开之后所有的排序或者聚合操作都将使用doc values.","link":"/2015/04/26/disk-based-field-data-doc-values/"},{"title":"GIT 提交时的特殊字符","text":"Intellij IDEA 偶尔在提交代码时候会出错, 产生包含特殊字符的路径和文件, 普通的办法无法删除 需要利用 -o -z 参数才能够正常添加到改动: git ls-files -d -m -o -z -x | xargs -0 git update-index –add –remove -o -z 包含特殊字符 -x 用于应用忽略列表","link":"/2015/01/11/git-special-words-path-remove/"},{"title":"MAC OS X 小技巧","text":"实现嗅探器 sudo tcpdump -A -s0 -ien3 port 9200","link":"/2015/02/24/mac-os-tips/"},{"title":"Remoting","text":"##RemotingAkka remoting 包括了一系列远程部署于监控相关功能, 这些功能均可以以配置方式实现也可以通过编程方式. Akka remoting 基于点对点的通讯模式所设计并且相对于客户端服务器模式有一些限制, 特别是 Akka Remoting 无法很好的使用包括 NAT 和 第三方负载均衡器以及其他的一些网络基础设施. ###让你的 ActorSystem 做好使用 Remoting 的准备Akka Remoting 是一个单独的 jar 包, 请确认这个如下的 jar 包已经作为依赖添加到项目的 sbt 文件中. &quot;com.typesafe.akka&quot; %% &quot;akka-remote&quot; % &quot;2.3.9&quot; 要在 Akka 项目中使用远程特性, 至少需要在 application.conf 中做出类似以下的修改. akka { actor { provider = &quot;akka.remote.RemoteActorRefProvider&quot; } remote { enabled-transports = [&quot;akka.remote.netty.tcp&quot;] netty.tcp { hostname = &quot;127.0.0.1&quot; port = 2552 } } } As you can see in the example above there are four things you need to add to get started:就像你再上面例子中看到的一样, 总共有四个配置需要添加: 将 provider 的类型从 akka.actor.LocalActorRefProvider 修改为 akka.remote.RemoteActorRefProvider 指定传输端口, 通常是akka.remote.netty.tcp 添加地址 - 就是你运行 Akka 应用程序的机器名或者 IP 地址, 机器名将用于标记本地系统绑定网卡使得本地的 Akka system 可以和远端通信, 并且远端也可以基于此地址连接到本地系统 添加端口号 - Akka remoting 侦听的端口号, 设置成0系统会自动寻找一个可用的端口 如果有超过一个 akka 系统在一台机器上, 则端口号必须互不相同, 即使是这些 akka 系统名称不相同, 这是因为每个 akka 系统进程都会进行自己的网络侦听和连接. 这个例子仅仅阐述了启用 remoting 所需的最低限度的配置, 其他相关配置, 会在文章末尾给出. ###使用 Remoting 的两种方法Akka 有两种办法来使用 Remoting 查找: 使用 actorSelection(path) 查找一个位于远程节点上的 actor 创建: 使用 actorOf(Props(...), actorName) 在一个远程节点上创建一个 actor 下面一个章节中我们将会对这两种方法进行详细计算. ###查找远程 ActoractorSelection(path) 将会返回一个 指向远端节点上 actor 的 ActorSelection, 例如: val selection = context.actorSelection(&quot;akka.tcp://actorSystemName@10.0.0.1:2552/user/actorName&quot;) 就像你在上面的例子中所看到的一样, 用以下的模式来找到在远端节点上的 actor: akka.&lt;protocol&gt;://&lt;actor system&gt;@&lt;hostname&gt;:&lt;port&gt;/&lt;actor path&gt; 一旦你获取到 actor 的 selection 之后, 你可以像本地节点一样的方式和它进行通讯: selection ! &quot;Pretty awesome feature&quot; 如果需要通过ActorSelection获取到相应的ActorRef你需要发送一条消息给那个 selection 并且在收到回复消息之后, 使用 sender 来获取对端的 ActorRef, 内置消息Identify是所有 actor 都能够理解并且自动以一个ActorIdentity(包含了对应的 ActorRef)消息回复发送者. 你也可以使用ActorSelection的resolveOne这个方法返回一个结果为ActorRef的Future 关于 actor 地址和路径更多的细节请参考相关其他文章 ###创建远端的 actor如果你希望以远端的方式来创建 actor 你可以在application.conf添加如下配置(这里仅仅是部署的部分) akka { actor { deployment { /sampleActor { remote = &quot;akka.tcp://sampleActorSystem@127.0.0.1:2553&quot; } } } } 上面的配置告诉 Akka 当一个路径为/sampleActor的 actor 被创建时, 比如使用system.actorOf(Props(...), &quot;sampleActor&quot;), 这个 actor 将不会被直接初始化, 取而代之的远端系统将会来创建这个 actor, 在这个例子中, 远端系统就是`sampleActorSystem@127.0.0.1:2553` 一旦你加入了上面的配置, 就可以直接像创建本地 actor 的方式进行创建了: val actor = system.actorOf(Props[SampleActor], &quot;sampleActor&quot;) actor ! &quot;Pretty slick&quot; 上面例子中的, actor 的类型SampleActor将在运行时被使用, 也就是说, 目标系统的 classloader 必须已经包含了这个类. 为了去确保 Props 在使用了传入构造函数参数的情况下能够正常的被序列化, 不要在使用内部类来作为工厂, 这回导致依赖内部类的容器类, 在大部分场景下, 这都将导致序列化失败, 或者意外的序列化开销, 最佳的方案是将工厂方法放在 actor 的伴生对象中.所有Props的可序列化性检测可以通过开启akka.actor.serialize-creators=on配置激活, 开启这个配置后, 只有本地部署 actor 可以避免被检测 你可以使用*作为通配符匹配 actor 的路径, 所以你可以用/*/sampleActor来匹配在这个路径下所有名为sampleActor的 actor, 你同样可以将通配符用在路径的最后一层/someParent/*, 没有任何通配符的匹配优先级高于任何有通配符的, 所以/foo/bar被认为比/foo/*更为精确, 所以前者生效而后者不生效. 另外需要注意的是, 部分匹配是不生效的, 例如/foo*/bar或者/f*o/bar ###使用编程方式来远程部署为了允许动态的部署, 因此也可以在 Props 上附加部署相关的配置用于远程创建actor, 这些额外的属性等价于配置文件中development这个节点下的内容, 如果通过变成方式增加了 deploy 信息而外部配置文件也具有相关信息, 则以外部配置文件为主. 你需要 import 以下包: import akka.actor.{ Props, Deploy, Address, AddressFromURIString } import akka.remote.RemoteScope 创建远程地址: val one = AddressFromURIString(&quot;akka.tcp://sys@host:1234&quot;) val two = Address(&quot;akka.tcp&quot;, &quot;sys&quot;, &quot;host&quot;, 1234) // this gives the same 以远程方式来创建 actor: val ref = system.actorOf(Props[SampleActor].withDeploy(Deploy(scope = RemoteScope(address)))) ###生命周期和错误恢复模型 所有远程系统的连接状态均为上图所表现出的四中状态中的一种, 在远程系统没有进行任何通讯之前, 它的状态都是Idle, 当消息第一次视图发送给远程系统, 或者一个入站连接被接受时, 这个 link 的状态将变成Active这意味着两个系统之间开始有消息发送或接受并且到目前为止没遇到什么问题, 当一次通讯失败(例如, TCP连接失败, 传输FD被触发?, 机器名解析失败, 远端系统关闭)并且两个系统之间的连接丢失了, 这时连接将会变为Gated. 在这个状态, 系统将不会试图链接远程系统, 并且所有出站的消息将会被放弃, 一个连接的Gated状态持续时间由akka.remote.retry-gate-closed-for这个配置进行控制, 在这个配置中的时间过去之后链接状态将会再次回到Idle, Gate 是一个在单边上的意义, 所以一旦一个来自于远端系统的入站链接被接受之后, Gate将会自动变成Active状态, 并且通讯将会立即恢复. 面对灾难性通讯失败(例如,远程系统死亡监控被触发, 系统级消息发送失败, 系统集群被移除的事件被侦听到), 这是无法恢复的, 因为相关系统的状态已经不一致了, 远端系统将会变成Quarantined, 不像Gate, Quarantined是永久性的, 知道其中一个系统重新启动, 在重新启动之后通讯将会被恢复, 而连接状态将会重新变成Active ###监控远程 actor监控远程 actor 和监控本地 actor 没什么区别, 请参相关文档. 监控一个由actorFor获取的ActorRef不会在失去连接的时候触发Terminated事件, actorFor已经过时, 需要用actorSelection来代替, 通过Identify消息来接受ActorIdentify消息在相关文档中已经描述. ####失败侦测在远程死亡监控中我们使用心跳消息并且在网络错误或者JVM崩溃时时创建一个Terminated,并且以平滑的方式终止所监控的actor. 心跳规则基于 The Phi Accrual Failure Detector. 错误产生的可能级别, 通过叫做phi的值来给定, phi错误侦测的基本概念就是基于当前网络状况动态的调整phi, phi的值通过以下公式计算: phi = -log10(1 - F(timeSinceLastHeartbeat)) F是一个基于平均值的正态分布具和从历史心跳间到达时间估计的标准偏差的累积分布函数, 在配置文件中remote部分你可以通过调整akka.remote.watch-failure-detector.threshold值来确定phi到达到何种程度时就视作错误发生. 如果将threshold设置的较低, 可能导致需要误报发生但是可以确保一旦有错误发生可以很快的被侦测到, 相对的, 一个较高的threshold值将产生较少的错误, 但是需要更多时间才能探测到一个真正的错误, 默认的threshold的值为10, 这应该适用于大部分场景, 但是在使用云端的场景, 比如Amazon EC2, 这个值可能需要被增加到12来避免这些平台经常出现的网络错误. 下面的表格模拟了, 当心跳时间开始增加时, phi值的变化. phi基于平均值和历史到达时间的标准偏差来进行计算, 上面的图表是一个标准偏差达到200毫秒的例子, 如果心跳抵达时间遍插值更小, 则取消变得更加陡峭, 换而言之, 这使得错误被识别出来的更快, 下图是标准偏差达到100毫秒的例子. 为了能够在突发性的异常中存活, 比如垃圾收集导致的暂停, 短暂的网络故障等, 可以调整remote配置中的akka.remote.watch-failure-detector.acceptable-heartbeat-pause. 下图展示了当acceptable-heartbeat-pause被设置到3秒的效果. ###序列化当使用远程actor的时候你必须确认这些 actor 所使用的props和message都是可以被成功序列化的, 否则可能会导致系统出一些意料之外的错误. 更多的细节请参阅akka序列化这部分文档. ###在 Router 中使用远程地址Router中使用remoting功能是完全可行的, 一个进行远程部署的pool可以像这样进行配置: akka.actor.deployment { /parent/remotePool { router = round-robin-pool nr-of-instances = 10 target.nodes = [&quot;akka.tcp://app@10.0.0.2:2552&quot;, &quot;akka://app@10.0.0.3:2552&quot;] } } 这个配置将会使用给定的Props在每个配置的节点上创建10个actor. 一个远程 actor 的 group 可以像这样配置: akka.actor.deployment { /parent/remoteGroup { router = round-robin-group routees.paths = [ &quot;akka.tcp://app@10.0.0.1:2552/user/workers/w1&quot;, &quot;akka.tcp://app@10.0.0.2:2552/user/workers/w1&quot;, &quot;akka.tcp://app@10.0.0.3:2552/user/workers/w1&quot;] } } 这个配置将会发送消息给所有定义的 actor, 这需要你在远程节点上预先创建好这些 actor, group router 不会替你来做这些事情 ###Remoting的例子Akka官方网站上有一些相关的例子可以查询. 请自行Google ####可插拔的传输层Akka Remoting被设计为可以使用各种不同的传输层实现来与其他远程系统通信, 核心组件功能通过akka.remote.transport.Transport定义, 传输层都必须实现这个特性, 传输层可以通过akka.remote.enabled-transports这个配置来进行加载, 相关的参数也可以包含在配置文件中. 一个Netty SSL传输层的例子. akka { remote { enabled-transports = [akka.remote.netty.ssl] netty.ssl.security { key-store = &quot;mykeystore&quot; trust-store = &quot;mytruststore&quot; key-store-password = &quot;changeme&quot; key-password = &quot;changeme&quot; trust-store-password = &quot;changeme&quot; protocol = &quot;TLSv1&quot; random-number-generator = &quot;AES128CounterSecureRNG&quot; enabled-algorithms = [TLS_RSA_WITH_AES_128_CBC_SHA] } } } 一个自定义传输层的例子 akka { remote { applied-transports = [&quot;akka.remote.mytransport&quot;] mytransport { # The transport-class configuration entry is required, and # it must contain the fully qualified name of the transport # implementation transport-class = &quot;my.package.MyTransport&quot; # It is possible to decorate Transports with additional services. # Adapters should be registered in the &quot;adapters&quot; sections to # be able to apply them to transports applied-adapters = [] # Driver specific configuration options has to be in the same # section: some-config = foo another-config = bar } } } ####远程事件监听远程系统中的事件是可行的, 并且只需要简单的在ActorSystem.eventStream中进行注册/反注册即可. 使用RemotingLifecycleEvent来注册所有远程事件, 使用akka.remote.AssociationEvent注册连接生命周期事件. 使用Association而不是Connection因为远程系统可能使用一些无连接的传输层, 一个association对于传输层可以使用akka系统非常容易的维护. 默认情况下一个侦听器将会将会注册所有的事件, 这个默认值非常有帮助你调整整个系统, 但是通常在生产环境可以通过配置关闭 要关闭logging, 将akka.remote.log-remote-lifecycle-events 设置为 off 相关事件: DisassociatedEvent 连接被关闭 AssociatedEvent 连接建立 AssociationErrorEvent 连接出错, 在需要直接介入错误处理时侦听 RemotingListenEvent 远程系统已经做好接受连接准备 RemotingShutdownEvent 远程系统已经关闭 RemotingErrorEvent remoting相关错误 略过安全性方面配置, 生产环境一般不会用到","link":"/2015/02/04/AKKA-Remoting/"},{"title":"相关度 - 相关度打分理论","text":"Theory behind relevance scoringLucene(包括 Elasticsearch) 使用 Boolean 模型来寻找匹配的文档, 并且使用名为Practical Scoring Function公式来计算相关度, 这个公式借用片段频率(Term Frequency)/逆文本频率指数(Inverse Document Frequency)和向量空间模型(Vector Space Model)的概念但是增加了更多新的功能比如协调因子(coordination factor), 字段长度统一化(field length normalization)和片段(term)增强( boosting).不要被吓住了, 这些概念并非像他们名字那样复杂, 虽然我们在这一节中会提到算法, 公式和数学模型, 但是我们会尽可能的”说人话”, 了解这些算法本身相对于了解这些因子对于结果不是那么重要. 布尔模型( Boolean Model)布尔模型在查询中简单的通过 AND,OR和 NOT 这样的条件表达式来寻找所有匹配的文档, 例如一个查询: full AND text AND search AND (elasticsearch OR lucene) 搜索到的文档必须包含 full, text, search, elasticsearch 或者 lucene 其中一个这些片段, 这个查询处理起来简单而快速, 用于在查询中快速排除任何不可能匹配的文档. 片段频率(Term Frequency)/逆文本频率指数(Inverse Document Frequency)一旦我们有了一个匹配的列表, 他们需要按照相关度进行排序, 不是所有文档都会包含所有的片段, 并且一些片段要比其他的片段重要一些, 文档的整体相关度分数取决于(部分)每个出现在文档中用以查询的片段的权重. 一个片段的权重取决于三个因子, 我们已经在之前章节中介绍过了, 公式包含了所有相关的因素, 但是你并没必要记住他们 片段频率一个片段在这个文档中出现了多少次? 越是出现的频繁, 那么权重越高, 一个字段提到了五次相同的片段就比这个字段只提到一次相关度更高, 我们使用如下公式来计算片段: tf(t in d) = √frequency 针对文档中的片段频率( tf) 通过对文档中片段出现次数开根号获得 如果你不关心一个字段中片段出现的频率, 比如你只关心这个片段是否在这个文档中出现过, 你可以在字段映射的时候屏蔽掉片段频率计算: PUT /my_index { &quot;mappings&quot;: { &quot;doc&quot;: { &quot;properties&quot;: { &quot;text&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;index_options&quot;: &quot;docs&quot; //(1) } } } } } 将 index_option 设置成 docs 将屏蔽片段频率和片段位置, 党一个字段使用这个映射将不会去计算片段出现了多少次, 并且不能在 phrase 或者 proximity 查询中使用, 用于精确匹配的 not_analyzed 字符串字段默认使用这个设置逆文本频率指数 在索引集合中这个片段在所有文档中有多常见, 越是常见, 权重越低, 常见的片段由于他们出现在大量文档中因此只能提供少量的相关度, 然而不常见的片段比如 elastic 或者 hippopotamus 这样的单词帮助我们聚焦于真正感兴趣的文档, 逆文本频率指数通过以下公式计算: idf(t) = 1 + log ( numDocs / (docFreq + 1))针对碎片的逆文本频率指数使用如下算法: 索引中文档总数除以包含这个片段文档数量(加以避免除0) 然后取对数 加上1 字段长度基准这个字段有多长, 越短的字段越是具备高权重, 如果一个片段出现在一个短的字段中, 比如标题字段, 片段更可能出现在一个较大的内容字段中比如”关于”, 字段长度基准使用下面的公式进行计算: norm(d) = 1 / √numTerms 字段长度基准等于1除以这个字段中片段总数开平方 虽然字段长度基准对于全文搜索非常重要, 但是有可能很多其他类型的字段不需要它, 字段长度基准在索引的时候对于每个字符串字段占用大约一个字节, 无论这个文档包含这个字段与否, 将字符串字段设置为 not_analyzed 将默认的屏蔽掉字段长度基准, 但是你也可以使用以下映射配置来屏蔽一个 analyzed 字段: PUT /my_index { &quot;mappings&quot;: { &quot;doc&quot;: { &quot;properties&quot;: { &quot;text&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;norms&quot;: { &quot;enabled&quot;: false } (1) } } } } } 这个字段将不会将字段长度基准作为考虑范围, 一个长的字段或者一个短的字段在计算分数时, 将视为同样的长度. 在一些场景下, 比如日志, 长度基准基本没有作用, 你所考虑的仅仅是这个字段是否包含一些错误代码或者浏览器识别信息, 字段的长度对预期结果没任何影响, 关闭长度基准可以稍微节省一些内存. 把他们放到一起这三个因素, 片段频率, 逆文本频率指数, 字段长度基准 都在索引时计算并存储, 同样的, 他们被用于计算单个片段在特定文档中的权重. 每当我们将上面的公式和文档放在一起说, 事实上我们说的是某个文档中的字段, 每个字段都有个逆索引因此对于 片段频率和逆文本频率指数来说, 字段的值就是正哥文档的值当我们运行一个简单的片段查询时, 我们可以将explain 设置为 true, 你将看到涉及分数计算的因子就是我们之前提到的那些: PUT /my_index/doc/1 { &quot;text&quot; : &quot;quick brown fox&quot; } GET /my_index/doc/_search?explain { &quot;query&quot;: { &quot;term&quot;: { &quot;text&quot;: &quot;fox&quot; } } } 上面的请求解释如下(为了看起来容易点, 我们简化了下结果): weight(text:fox in 0) [PerFieldSimilarity]: 0.15342641 //(1) result of: fieldWeight in 0 0.15342641 product of: tf(freq=1.0), with freq of 1: 1.0 //(2) idf(docFreq=1, maxDocs=1): 0.30685282 //(3) fieldNorm(doc=0): 0.5 (4) 内部 ID 为0的文档中, 字段 text 在查询片段为fox 的时候的最终得分为0.15342641 片段fox在 text 中只出现了一次, 因此 TF 结果为1 逆文本频率fox这个片段在所有文档中出现过一次, 因此 IDF结果为 0.30685282 当然一个查询通常不只是由一个片段组成, 所以我们需要找到一个办法来合并多个片段的结果, 因此我们引入空间向量模型(VSM) 空间向量模型空间向量模型提供了在多片段查询的情况下给了我们一个途径来对比文档的手段, 结果是一个能体现出文档相关度的分数, 为了做到这一点, 通过使用向量来表现文档和查询. 一个向量看起来就像一个包含数字的一位数组, 例如: [1,2,5,22,3,8] 在空间向量模型中, 想两种每个数字都代表一个片段的权重, 通过 TF/IDF 计算得出. 虽然 对于空间向量模型TF/IDF 是计算片段权重的的默认方法, 但不是唯一的方法, 其他的一些模型例如 Elasticsearch 支持的 Okapi-BM25同样也可以做到相同的事情. TF/IDF 之所以成为默认的手段, 因为他足够简单, 高效的算法能够提供更高的查询速度并能够降低测试时间.想想下, 如果我们查询” happy hippopotamus”, 一个常见的词汇比如 happy 只拥有低权重, 让我们假设 happy 的权重为2, 而 hippopotamus 的权重为5, 让我把他们设为一个简单的二维向量[2,5] 在二维坐标系中表现起来就是一根线, 起点为0,0 而终点为2,5 现在, 假设我们有三个文档, “I am happy in summer” “After Christmas I’m a hippopotamus” “The happy hippopotamus helped Harry” 我们可以为每个文档依据查询的片段权重创建一个简单的向量. 然后把他们放到同一张图上去比较. [2,0] [0,5] [2,5] 将查询和文档向量放到一个图上就便得非常容易进行比较, 通过测量查询向量和文档向量之间的角度就有能够给每个文档计算出相关度的值, 文档1的角度和查询相比相差太大, 文档2稍微接近一点, 而文档3 则是完美匹配. 事实上, 二维向量(根据两个片段进行查询), 可以很简单的用图来表示, 幸运的是线性代数, 用来处理向量的数学分值, 提供了工具让我们来对比多维向量, 这意味着我们可以可以处理多片段的查询. 你可以去看看如何使用余弦相似度(Cosine Similarity)http://en.wikipedia.org/wiki/Cosine_similarity 现在我们已经讨论完毕关于评分的理论基础, 我们可以去看看 Lucene 是如何实现评分的了.”","link":"/2014/11/01/EDG-Relevance-Scoring_theory/"},{"title":"Hbase 经验总结","text":"使用 Hbase 一段时间之后根据网上找到的一些资料和这段时间总结下来的经验, Hbase 是一个经过各个大公司使用并确认, 是目前在可靠性, 性能, 开发难度, 可扩展性方面都非常不错的工具, 针对二维索引目前也有使用协处理器或者表内索引的办法, 但都无法完美解决, 通常会使用 Elasticearch 或 Solr(甚至是Mongodb)对数据进行索引, 在博客中我会陆续提到这些技术. ##线上关注点 CPU 负载状况 内存使用情况 网络流量(In/Out/RPC) 磁盘 IO 状况 读写请求响应时间, 读写请求次数, 读写请求分布/ GET/ NEXT 等 Region 数量/ Region Block Cache命中率 / Compaction 队列 / Flush 次数 / Split 次数 / Compact次数 / 最大请求时间 / Hlog 数量 / Block 本地化率 / Compact 流量 / Flush 流量 等 ##日志 Hbase 日志 HDFS 日志 GC 日志 YGC / CMS / FGC (尤其是 FGC 要特别关注消耗时间)服务端和客户端异常 (请求超时, 延时过高等) ##监控工具 Hbase WebUI, 仅提供了最基本的信息 Ganglia, 有点老了, 但是依然好用, 比 WebUI 强大 Cloudrea Manager, 推荐 自己开发的监控程序, 例如对远程同步时延监控等. ##工具 jmap / jstack / jconsole 关注 JVM 运行时 btrace, 运行环境调试, 动态注入字节码(注意, 不正确使用可能导致 jvm 崩溃, 先本地测试通过之后, 在使用到生产环境) java profiler (jrockit / yourkit java profiler) jvm 运行状态监控, 比 jvm 自带的强大很多倍 Hbase Shell, 必须要熟悉 RegionSplitter / graceful_stop.sh / HFile / HLog / hbck / copytable / export / import/ distcp / bulk load 等至少要知道干嘛的 ##系统指令top / sar / vmstat / iptrac / df 这些都是最基本的 ##可能会遇到的问题 version设置过多, blockcache, bloomfilter 类型不正确, storefile 过多, 命中率过低解决的办法是, 分析 HFile, 看是否结果太大, 查看 HDFS看看是不是有一些块读取慢或者磁盘故障 客户端写请求大量出错Region offline, split 时间过长, meta 表错误, Full GC, GC已经在使用 swap 交换分区, Load 过高, 磁盘压力过大, 磁盘故障, 这些需要配合监控程序找到问题, 看是加机器还是替换掉有问题的硬件, HDFS 出错, 内存上限, Request queue 堆积, 看看堆栈的情况 系统速度越来越慢Flush compact过于频繁, 磁盘 IO 下降, Region的 storefile个数太多, region 不平衡命中率下降, 节点故障 数据丢失Hbck检查下region 的分配情况, region server abort 日志是否有正确恢复, log / oldlogs 看看数据是否写入了 服务器进程挂了FGC / YGC 时间太长, OOM, fatal 日志 ##一些主要但是容易被忽略的配置 直接关闭操作系统的swap 或者设置 swapiness为0, 一旦 GC 使用到了 swap 基本就要糟 CMS GC, 默认为90%, 建议设置到65%-75% 通过配置mslab 可以在一定程度上延缓 FGC 发作的时间.(仅仅是延缓), 使用BucketCache 解决 FGC 的问题(以提高 YGC 频率降低单词 YGC 的时间为代价, 在 taobao 被证明非常有效) zookeeper.session.timeout default 3 minutes, GC 时间不能超过ZK session 过期时间, 一旦超过会触发balance, 调高或者调低取决于集群负载状况, 在低负载情况下个人建议调低, 以便节点出现问题 hbase 能够迅速处理, 但在集群负载过高的情况下, 建议调高, 避免因为频繁 balance 导致集群负载雪上加霜 hbase.regionserver.handler.count default 10, 较小的线程数比较适合 big put, 较多的线程适合内存少, TPS高的场景, 总之要对内存造成压力,这里需要注意的是如果server的region数量很少，大量的请求都落在一个region上，因快速充满memstore触发flush导致的读写锁会影响全局TPS，不是IO线程数越高越好。 压测时，开启Enabling RPC-level logging，可以同时监控每次请求的内存消耗和GC的状况，最后通过多次压测结果来合理调节IO线程数。这里有一个案例,Hadoop and HBase Optimization for Read Intensive Search Applications(请谷歌)，作者在SSD的机器上设置IO线程数为100，仅供参考。 hbase.hregion.max.filesize default 256mb 需要加大(比如加到到100G 等于间接关掉自动 split), 减少 split, 在发现文件过大的时候手工进行split 使用 regionSplitter( 可以通过自己开发的工具来自动进行, 避开业务高峰时段),Compaction 和 split 是性能杀手, 切记! hbase.hregion.memstore.block.multiplier 这个一般情况下默认值还是比较靠谱的, 不要乱改不要在一张表里定义太多的Column Family, Hbase目前不能良好的处理超过包含2-3个CF的表。因为某个CF在flush发生时，它邻近的CF也会因关联效应被触发flush，最终导致系统产生更多IO。在批量导入数据到Hbase前，你可以通过预先创建regions，来平衡数据的负载。详见? Bloom Filter以空间换时间, 一定要用 ##踩过的坑 Region Server OOM 现象region server 出现 OOM 原因Compaction 占用多大内存, 行数据过大(version 太多, 或者表太宽) 解决办法内存用量=Memstore+Blockcache+其他, 问题通常出在其他, 行数据尽量控制在1m 以内, 优化 compaction, 通过调整之前提到的hbase.hregion.max.filesize NotServingRegionException 现象压力大时, Hbase不可用, 读写端周期性出现异常 原因通常我们对时效性数据或者带时间字段的数据会吧时间编入 key 中, 因此每天(或者隔一段时间)就会创建新的 region, 由于写入数据量较大, 进一步触发了Hbase的Region Split 操作, 这个操作耗时很长(平均10秒的样子, 当时大小设置的时4G), 而且触发的 Region split 非常频繁, Region split 会导致region 分布不均匀, 从而触发hbase 自动做 region rebalance 操作, region进行迁移的时候这个 region 会下线, 时间还比较长(一般都要20秒吧) 解决办法Client 重试(…), 对Rowkey pre-split( 分片) , 减少hbase的自动split, 或者干脆 rowkey 设计成 ttl, 直接关掉 balance, 低峰时间段开启(真的, 后来我们发现这个方法最有效), 总的来说 region 不可用通常都是短时间的非致命性问题, 但是可能会对服务产生不良影响, 需要尽可能避免 Compaction和Scan 变慢 现象一个 Hbase 表, 每天会定时导入一批数据(和我们将来的场景类似), 数据量很大, 千万级别, 然后在其他时段为用户提供服务, 某天突然发现这个表按去 scan 少量数据部分 region非常慢, 需要十多秒甚至超过一分钟 原因从监控程序上来看, 这个表下面只有几个Storefile, 所以派出了 storefile 过多导致速度降低的原因, 最后问题是发现大量的重复数据在被进行覆盖, 但是未进行 compact 导致过去数据未被清除 解决每天导入数据之后, 强制执行major compact, 调小major compact周期, 指定时间段运行, 相关配置, hbase.hregion.majorcompaction / hbase.hstore.compaction.ratio, hbase.offpeak.start.hour / hbase.offpeak.end.hour / hbase.hstore.compaction.ratio.offpeak, hbase.regionserver.thread.compaction.large / hbase.regionserver.thread.compaction.small / hbase.regionserver.thread.compaction.throttle ##用 Zookeeper 管理多个集群Zookeeper 是协调器, 轻量级, load 很低, 多个 hbase 通过 zookeeper.znode.parent配置到不同目录使用同一个 zookeeper 集群在集群多的时候提高运维效率, 节约点服务器 ##关于数据导入需求个人不建议使用replication, 没用过, 而且听说坑比较多, (Hbase 里面你看起来那些越智能, 越方便的功能, 通常使用下来会发现非常坑爹)建议使用bulkload, 原因是速度非常快, 适合导入场景, 在建表的时候一定要做好region 预切分, HFileOutputFormat.configureIncrementalLoad方法会根据region 数量来决定 reduce的数量和每个 reduce覆盖的 rowkey 范围, 否则某个 reduce任务过多, 降低导入性能单个rowkey 下子列不要太多, 否则reduce 阶段排序会出现 oom, 你们也可以通过二次排序避免 reduce 去排序, 到时候看bulkload 执行完毕之后需要把 hdfs 中生成好的文件写入到hbase 表中, 采用hadoop jar hbase- version.jar completebulkload /hfilepath tablename 命令实现。hadoop-lzo jar,LzoTextInputFormat, 黑科技, 支持 lzo 数据导入, 避免自定义压缩格式导入导出之前的解压缩操作 ##BlockCache 缓存(吐槽)默认的blockcache 实现, 使用一个 hashmap 来维护 block key -&gt; block 的映射关系, 采用严格的LRU算法(有兴趣自己 google)来进行淘汰, 初始化会指定容量大小, 当使用量达到85%就开始淘汰block到75%好处是直接使用jvm 的 hashmap, 简单, 内存用多少算多少, jvm 会帮你回收你释放的 block(by GC)坏处一个 block从缓存到淘汰, 在 heap 中得位置从 new 到 old, 然后 old 区的缓存被淘汰之后, 就由 CMS 来回收, 然后就是 heap 碎片的问题, 又因为碎片的问题, 导致 FGC, 根据应用的情况, FGC 发生频率可能一个月, 一周, 甚至半年一天都可能, 所以避免作死, 最好还是低峰期手工来 GC如果缓存的速度比淘汰的速度快, oops, 很有可能OOM(就挂了) ##Hbase 的 GC和女人心情一样, 随时都有可能出现的 FGC 会导致hbase 集群性能急剧下降, 导致客户端操作更严重超过 zookeeper 的超时时间导致 zookeeper 把这个 region 下线(从而导致后续一系列悲剧), blockcache 大量加载的时候(载入 block 到内存)出现的 ygc时间可能超过500ms, 个别情况 ygc 会长期保持在200ms 水平, 你在性能测试的时候会看到毛刺就是因为这个典型的配置jvm head通常会开到34G, 其中 new 会设置到2-4G, CMS 发生比率配置在75%根据应用场景不同, 如果要避免内存碎片过多, 会把 CMS 设置到65%, 这样其实牺牲了LRUcache 的大小, 影响了部分性能.Hbase 出现FGC 绝大多数原因是因为内存碎片, 大部分场景是在连续混合读写后, 如果瞬间出现大量低命中读, 并伴随大量blockcache 加载, 就很容易出现. ##Bloomfilter 是个好东西为数不多看起来智能好用又不坑的东西, 提高随机读取的性能,对顺序读取没啥帮助, 在生成Storefile 的时候会包括 bloomfilter信息, metablock, 存在 blockcache 中, 对于某个 region 的随机读, hbase会遍历读memstore和 storefile, 将结果merge 后返回给客户端, 如果你设置了 bloomfilter, 那么在读取storefile 的时候就可以直接跳过一些storefile. region下的 storefile 越多 bloomfilter 效果越好 region 下得 storefile 越少 hbase 读取性能就越好 ##毁灭性事故某次运维将hbase.hregion.max.files设置成0了, 导致 region 无限制 split, 导致region offline, 从而你没法去挽救这种情况解决办法就是到HDFS 中删除.META 文件/HDFS 文件/ZK 文件, 然后重启集群只能挽救集群, 删除的文件中数据想办法恢复 ##Region server compact 占用大量带宽Hbase 集群运行过程中部分 region server 直接将网络 IO 跑满 千兆网卡*2 同时机器负载也非常高最后发现是大量写入造成频繁compaction解决的办法就是减少 compaction","link":"/2014/11/21/Hbase_practice/"},{"title":"Solr的地理空间索引","text":"Solr 支持基于空间或者地理空间使用位置数据进行搜索, 使用空间搜索, 可以做到:索引点位或者其他的形状通过边界框,圆形或者其他形状来过滤搜索结果基于两点间距离或者两个方形之间的相关区域进行排序和提高分数索引和搜索具备多个值的时间或者其他数字范围 在 Solr 4中有三种字段类型用于支持空间搜索: LatLonType 或者是非地理坐标 PointType SpatialRecursivePrefixTreeFieldType(RPT) BBoxFieldLatLonType 是在 Solr 3 中被引入的第一种空间数据类型, 其他的几种在这之后被引入, RPT 提供了比 LatLonType 更多的功能和更快的过滤速度, 但是当有距离排序/优化的需求时 LatLonType 依然是最适合的. 他们可以被同时使用, LatLonType 用于排序和优化, RPT 用于过滤, 这样可以获得最好的效果, BBoxField用于边界框索引, 提供了一些谓语(相交,是否在..内,包含,于…不相交)进行搜索, 也可以使用类似overlapRatio或者简单的一个区域进行优化和排序, 更多信息可以参考http://wiki.apache.org/solr/SpatialSearch. ##索引与过滤 对于地理位置的索引(纬度与经度), 支持纬度于经度以两个数字用逗号分隔的字符串方式进行设置, 针对非地理位置坐标PointType, 顺序是 x,y, 如果是使用 RPT你必须使用空格代替逗号, 或者使用WKT 针对SpatialRecursivePrefixTreeFieldType后续后章节专门描述 ##空间过滤 以下参数用于进行空间搜索:d =&gt; 放射距离(半径), 单位是公里(针对 RPT 字段单位则是度)pt =&gt; 中心点, 使用”lat,lon”来表示经纬度, 如果使用 PointType 则是 “x,y”, 如果使用RPT 则是 “x y” ###geofilt geofilt 过滤器允许你基于点位和地理空间距离获取数据(简单来说, 就是一个圆圈), 另一种办法是使用一个圆形过滤器, 例如搜索给定经纬度五公里以内的数据, 你可以输入 &amp;q=:&amp;fq={!geofilt sfield=store}&amp;pt=45.15,-93.85&amp;d=5, 这个过滤器将会返回给定坐标和半径内的所有数据: ###bbox bbox过滤器非常类似于 geofilt 除了它使用一个边界框来代替之前的圆形, 参考途中蓝色的框, 它使用和 geofilt 一样的参数结构, 例如 &amp;q=:&amp;fq={!bbox sfield=store}&amp;pt=45.15,-93.85&amp;d=5, 矩形计算速度更快所以在有些时候如果能够接受一些点在半径之外的情况你可以用它来代替 geofilt, 然而, 如果你真的非常需要一个圆形的结果集并且希望它计算速度更快, 替代性的方式是使用 RPT字段并且尝试一个较大的 distErrPct 值例如0.1(10% radius). 这将使得结果集稍微超出预期的结果. 注意: 当一个边界框包括了一个极点(北极, 南极, 或者上边界, 下边界), 边界框将变成边界球(球冠)的形式, 如果它包括了北极, 将包括所有最低纬度以北的值(如果是南极, 将包括从最高纬度以南所有的值) 其实这段我也没看太懂… ###根据矩形进行过滤 有时空间搜索需要查找一个矩形区域内的所有数据, 就像一个区域被以地图的方式展现给用户, 在这种情况下 geofilt 和 bbox 无法满足需求. 这看起来有点像小技巧, 你可以使用”左上角-右下角”的范围查询语法, 例如: &amp;q=:&amp;fq=store:[45,-94 TO 46,-93]. LatLonType 不支持矩形范围跨越国际日期变更线(经度0), 但是 RPT 可以, 如果你使用 RPT的非地理位置坐标系(geo=“false”), 你必须将点位用双引号包含起来, 例如 “x y” ###优化: Solr Post Filtering 非常相似, 通过 RPT 字段类型可以做到最快速度的空间过滤, 然而, 有时当空间查询不值得缓存或者并基于一些其他的查询条件导致没有太多结果的时候, 使用在LatLonType 字段类型上使用Solr post filtering速度会更快一些, 如果要在 LatLonType 字段类型上使用Solr post filtering, 使用bbox 或者 geofilt 查询字符串并且增加参数cache=false 和 cost=100(或者更大), 这里有一个例子: &amp;q=...mykeywords...&amp;fq=...someotherfilters...&amp;fq={!geofilt cache=false cost=100}&amp;sfield=store&amp;pt=45.15,-93.85&amp;d=5 ##距离查询 总公共有四个距离查询方法:geodist, 参考下文, 这通常是最常用的;dist, 计算多维聚簇向量的 p- 范分布距离;hsin, 计算球面上两点之间距离;sqedist, 计算两点之间的欧氏距离;具体使用方法以及方法定义请查阅文档中 Function Queries 部分 ###geodist geodist 距离方法需要三个可选参数(sfield,latitude,longitude), 你可以是用geodist 对结果进行排序和计算分数, 例如:对结果进行距离反响排序: &amp;q=*:*&amp;fq={!geofilt}&amp;sfield=store&amp;pt=45.15,-93.85&amp;d=50&amp;sort=geodist sac 使用距离来结算结果分数: &amp;q={!func}geodist()&amp;sfield=store&amp;pt=45.15,-93.85&amp;sort=score+asc ##更多的例子 这里有一些有用的例子来告诉你, Solr 可以进行什么样的空间搜索 ###使用子查询来扩展查询结果 我们将查询 Florida州的Jacksonville市, 或者在45.15,-93.83(在Minnesota州Buffalo附近)50公里以内的数据: &amp;q=*:*&amp;fq=(state:&quot;FL&quot; AND city:&quot;Jacksonville&quot;) OR {!geofilt}&amp;sfield=store&amp;pt=45.15,-93.85&amp;d=50&amp;sort=geodist()+asc ###根据距离进行Facet 对距离进行 facet, 你可以使用 Frange 查询: &amp;q=*:*&amp;sfield=store&amp;pt=45.15,-93.85&amp;facet.query={!frange l=0 u=5}geodist()&amp;facet.query={!frange l=5.001 u=3000}geodist() 这里还有一种其他的办法可以做到同样的事情, 比如在每个 facet.query 使用{!geofilt} ###增强最近的结果 使用 DistMax 或者 Extended DisMax, 你可以通过使用 boost function 通过并空间搜索结果来增强距离最近的结果: &amp;q.alt=*:*&amp;fq={!geofilt}&amp;sfield=store&amp;pt=45.15,-93.85&amp;d=50&amp;bf=recip(geodist(),2,200,20)&amp;sort=score desc ##SpatialRecursivePrefixTreeFieldType (RPT) 这是Solr4 新的空间字段, 通过提供了一些新功能相对于 LatLonType 进行了改进:除了圆形和矩形之外, 额外支持多边形和更复杂的形状查询多个被索引的字段支持非点状(例如多边形)的索引, 就像点状索引一样, (意思就是不急于点, 数据可以指向一个范围)矩形可以跨越0经度(国际日期变更线), 在具备user-specified corners的情况下多个值的距离排序和分数修正Well-Known-Text (WKT) 形状语法(用于支持多边形和其他复杂的形状)RPT 包括了 LatLonType 和 PointType 的基础功能, 类似 矩形边界和圆圈, 事实上你应该使用 geofilt,bbox,geodist 和 range-query 这些方法在 LatLonType 和PointType 上 ###Schema 定义 要使用 RPT, 字段类型必须在 schema.xml 中注册, 这里提供了很多选项. name =&gt; 字段名称 class =&gt; 应该是solr.SpatialRecursivePrefixTreeFieldType units =&gt; 这个是必须制定的, 当前仅支持” degrees”, 无法用于 geofilt,bbox或者geodist(他们都是用的公里), 同样用于maxDistErr字段 distErrPct =&gt; 定义一个默认的非点状经度(同时用于索引和查询), 使用一个小数, 从0.0(完全精确) 到 0.5, 越接近0形状就约精确, 然而精确的索引形状将会消耗更多磁盘空间和使用更多时间进行索引, 更大的 distErrPct 值将会提高查询速度但降低精确度 maxDistErr =&gt;定义索引数据的最高细节, 如果留空默认值将会是1米, 比0.000009度稍微小一点, 这个设置将用于计算一个合适的最大距离geo =&gt; 如果设置为true, 也就是默认值, 经纬度坐标系将会适用于球形桌面, 如果是 fase 则仅局限于2D 坐标系 worldBounds =&gt; 定义x 和 y 的数字范围, 格式为ENVELOPE(minX, maxX, maxY, minY), 如果 geo = true, 则标准的世界经纬坐标系范围江北限制, 如果 geo = false 则你需要自己定义一个 distCalculator =&gt; 定义距离计算算法, 如果 geo = true, “haversine” 将是默认值, 如果 geo = false “cartesian” 将会是默认值, 另外一些可能的值包括”lawOfCosines”, “vincentySphere” 和 “cartesian^2” prefixTree =&gt; 定义空间网格实现, 因为 Prefixtree将世界映射为一个网格, 每一个cell都会包含其下一层其他的所有cell, 如果 geo = false 那么默认的 prefix tree 则为 “geohash” 其他情况为 “quad”, geohash在每级上包含32个子, quad 有8个, geohash 无法用于 geo=false , 因为他是一个严格的地理空间概念 maxLevels =&gt; 设置最大层级深度, 通常更直观的计算合适的 maxLevels 通过定义 maxDistErr&lt;fieldType name=&quot;location_rpt&quot; class=&quot;solr.SpatialRecursivePrefixTreeFieldType&quot; spatialContextFactory=&quot;com.spatial4j.core.context.jts.JtsSpatialContextFactory&quot; autoIndex=&quot;true&quot; distErrPct=&quot;0.025&quot; maxDistErr=&quot;0.000009&quot; units=&quot;degrees&quot; /&gt; 当字段类型定义好了, 我们就定义一些字段来使用它们.因为 RPT 具备一些更加先进的功能, 其中一些功能是新的或者实验性质的, 请查看http://wiki.apache.org/solr/SolrAdaptersForLuceneSpatial4以获取更多细节","link":"/2014/11/26/Solr_spatial_search/"}],"tags":[{"name":"ElasticSearch","slug":"ElasticSearch","link":"/tags/ElasticSearch/"},{"name":"EDG","slug":"EDG","link":"/tags/EDG/"},{"name":"Akka","slug":"Akka","link":"/tags/Akka/"},{"name":"Elasticsearch","slug":"Elasticsearch","link":"/tags/Elasticsearch/"},{"name":"Solr","slug":"Solr","link":"/tags/Solr/"},{"name":"BigData","slug":"BigData","link":"/tags/BigData/"},{"name":"git","slug":"git","link":"/tags/git/"},{"name":"Hbase","slug":"Hbase","link":"/tags/Hbase/"}],"categories":[]}